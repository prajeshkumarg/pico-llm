{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af008cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gprajeshkumar/Documents/NYU Courant/Machine Learning/hws/pico-llm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/gprajeshkumar/Documents/NYU Courant/Machine Learning/hws/pico-llm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "\n",
    "print('All imports done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab9e0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training and models\n",
    "# Adjust these as needed for your experiments\n",
    "\n",
    "# Data parameters\n",
    "block_size = 128  # Sequence length for training\n",
    "train_subset_size = 20000  # Number of samples to use from TinyStories\n",
    "batch_size = 16\n",
    "\n",
    "tinystories_weight = 0.5  # Proportion of TinyStories in mixed dataset (0.0 to skip)\n",
    "\n",
    "# Model parameters (default values, override in model cell if needed)\n",
    "mlp_k = 3  # K for KGramMLPSeqModel\n",
    "mlp_embed_size = 512\n",
    "mlp_num_inner_layers = 1\n",
    "mlp_chunk_size = 1\n",
    "\n",
    "lstm_embed_size = 512\n",
    "lstm_hidden_size = 512\n",
    "\n",
    "transformer_d_model = 256\n",
    "transformer_n_heads = 4\n",
    "transformer_n_blocks = 2\n",
    "transformer_max_seq_len = block_size\n",
    "\n",
    "# Training parameters\n",
    "epochs = 2\n",
    "learning_rate = 1e-3\n",
    "log_steps = 100\n",
    "sample_interval = 30  # seconds between text samples during training\n",
    "max_steps_per_epoch = None  # Set to an int for quick tests\n",
    "\n",
    "# Prompt for text generation\n",
    "default_prompt = \"Once upon a time\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6dbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_token_loss(logits, tokens_seq):\n",
    "    '''\n",
    "    logits: (seq_len, batch, vocab_size)\n",
    "    tokens_seq: (seq_len, batch)\n",
    "    Computes cross-entropy loss for next-token prediction.\n",
    "    '''\n",
    "    seq_len, batch = tokens_seq.shape\n",
    "    # Predict next token: input t => predict t+1\n",
    "    logits = logits[:-1]  # (seq_len-1, batch, vocab_size)\n",
    "    targets = tokens_seq[1:]  # (seq_len-1, batch)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf14e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device to use Apple Silicon GPU if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ee04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories from huggingface with weight=0.5...\n",
      "TinyStories sequences: 20000\n",
      "DataLoader ready. Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "# Parameters (set these as needed)\n",
    "block_size = 128\n",
    "train_subset_size = 20000  # or smaller for quick tests\n",
    "tinystories_weight = 0.5   # set to 0.0 to skip TinyStories\n",
    "\n",
    "# Load tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = enc.n_vocab\n",
    "\n",
    "# Load TinyStories dataset\n",
    "tinystories_seqs = []\n",
    "if tinystories_weight > 0.0:\n",
    "    print(f\"Loading TinyStories from huggingface with weight={tinystories_weight}...\")\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    dataset = dataset.select(range(train_subset_size))\n",
    "    for sample in dataset:\n",
    "        text = sample['text']\n",
    "        tokens = enc.encode(text)\n",
    "        tokens = tokens[:block_size]\n",
    "        if len(tokens) > 0:\n",
    "            tinystories_seqs.append(tokens)\n",
    "    print(f\"TinyStories sequences: {len(tinystories_seqs)}\")\n",
    "else:\n",
    "    print(\"TinyStories weight=0 => skipping TinyStories.\")\n",
    "\n",
    "# Optionally, load your own text files (uncomment and set file paths)\n",
    "other_seqs = []\n",
    "# input_files = [\"myfile.txt\"]\n",
    "# for filepath in input_files:\n",
    "#     with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue\n",
    "#         tokens = enc.encode(line)\n",
    "#         tokens = tokens[:block_size]\n",
    "#         if len(tokens) > 0:\n",
    "#             other_seqs.append(tokens)\n",
    "# print(f\"Custom input files: {len(other_seqs)} sequences loaded.\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class MixedSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tinystories_seqs, other_seqs, p_tiny: float):\n",
    "        super().__init__()\n",
    "        self.tinystories_seqs = tinystories_seqs\n",
    "        self.other_seqs = other_seqs\n",
    "        self.p_tiny = p_tiny\n",
    "        self.has_tinystories = (len(self.tinystories_seqs) > 0)\n",
    "        self.has_other = (len(self.other_seqs) > 0)\n",
    "        self.total_length = len(self.tinystories_seqs) + len(self.other_seqs)\n",
    "        if self.total_length == 0:\n",
    "            raise ValueError(\"No data found! Both TinyStories and other sets are empty.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import random\n",
    "        r = random.random()\n",
    "        if self.has_tinystories and self.has_other:\n",
    "            if r < self.p_tiny:\n",
    "                i = random.randint(0, len(self.tinystories_seqs) - 1)\n",
    "                seq = self.tinystories_seqs[i]\n",
    "            else:\n",
    "                i = random.randint(0, len(self.other_seqs) - 1)\n",
    "                seq = self.other_seqs[i]\n",
    "        elif self.has_tinystories:\n",
    "            i = random.randint(0, len(self.tinystories_seqs) - 1)\n",
    "            seq = self.tinystories_seqs[i]\n",
    "        else:\n",
    "            i = random.randint(0, len(self.other_seqs) - 1)\n",
    "            seq = self.other_seqs[i]\n",
    "        return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "def seq_collate_fn(batch):\n",
    "    max_len = max(len(seq) for seq in batch)\n",
    "    batch_size = len(batch)\n",
    "    padded = torch.zeros(max_len, batch_size, dtype=torch.long)\n",
    "    for i, seq in enumerate(batch):\n",
    "        seq_len = seq.size(0)\n",
    "        padded[:seq_len, i] = seq\n",
    "    return padded\n",
    "\n",
    "# Create dataset and loader\n",
    "p_tiny = tinystories_weight\n",
    "combined_dataset = MixedSequenceDataset(\n",
    "    tinystories_seqs=tinystories_seqs,\n",
    "    other_seqs=other_seqs,\n",
    "    p_tiny=p_tiny\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=seq_collate_fn\n",
    ")\n",
    "\n",
    "print(\"DataLoader ready. Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299ce0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGramMLPSeqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    For each position t in [0..seq_len-1], gather the last k tokens => one-hot => MLP => logits.\n",
    "    Return (seq_len, batch, vocab_size).\n",
    "\n",
    "    Potentially very large memory usage for big vocab or seq_len. chunk_size helps mitigate overhead.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, k=3, embed_size=1024, num_inner_layers=1, chunk_size=1, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_inner_layers = num_inner_layers\n",
    "        self.chunk_size = chunk_size\n",
    "        self.activation = activation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.k * self.vocab_size, self.embed_size),\n",
    "            self.activation(),\n",
    "            *[\n",
    "                layer for _ in range(self.num_inner_layers) for layer in (\n",
    "                    nn.Linear(self.embed_size, self.embed_size),\n",
    "                    self.activation()\n",
    "                )],\n",
    "            nn.Linear(self.embed_size, self.vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens_seq):\n",
    "        \"\"\"\n",
    "        tokens_seq: (seq_len, batch)\n",
    "        return: (seq_len, batch, vocab_size)\n",
    "        We'll do a loop over time steps. chunk_size can reduce overhead.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = tokens_seq.shape\n",
    "        outputs = []\n",
    "\n",
    "        start = 0\n",
    "        while start < seq_len:\n",
    "            end = min(start + self.chunk_size, seq_len)\n",
    "            block_outputs = []\n",
    "            for t in range(start, end):\n",
    "                batch_logits = []\n",
    "                for b in range(batch_size):\n",
    "                    if t < self.k:\n",
    "                        needed = self.k - t\n",
    "                        context_ids = [0]*needed + tokens_seq[:t, b].tolist()\n",
    "                    else:\n",
    "                        context_ids = tokens_seq[t-self.k:t, b].tolist()\n",
    "\n",
    "                    context_oh = F.one_hot(\n",
    "                        torch.tensor(context_ids, dtype=torch.long, device=tokens_seq.device),\n",
    "                        num_classes=self.vocab_size\n",
    "                    )\n",
    "                    context_flat = context_oh.flatten().float().unsqueeze(0)\n",
    "                    logits_b = self.net(context_flat)  # (1, vocab_size)\n",
    "                    batch_logits.append(logits_b)\n",
    "                block_outputs.append(torch.cat(batch_logits, dim=0).unsqueeze(0))  # (1, batch, vocab_size)\n",
    "\n",
    "            block_outputs = torch.cat(block_outputs, dim=0)  # (chunk_size, batch, vocab_size)\n",
    "            outputs.append(block_outputs)\n",
    "            start = end\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)  # (seq_len, batch, vocab_size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5fc50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=1024, hidden_size=1024):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=False)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, tokens_seq):\n",
    "        \"\"\"\n",
    "        tokens_seq: (seq_len, batch)\n",
    "        => (seq_len, batch, vocab_size)\n",
    "        \"\"\"\n",
    "        emb = self.embedding(tokens_seq)   # (seq_len, batch, embed)\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(emb)           # (seq_len, batch, hidden)\n",
    "        logits = self.linear(out)         # (seq_len, batch, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1a7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.use_bias = bias\n",
    "        self.bias = nn.Parameter(torch.zeros(dim)) if bias else None\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        y = (x / rms) * self.weight\n",
    "        if self.use_bias:\n",
    "            y = y + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention with optional KV cache.\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def _split_heads(self, t: torch.Tensor, T: int, B: int) -> torch.Tensor:\n",
    "        return t.view(T, B, self.n_heads, self.head_dim).permute(1, 2, 0, 3)  # (B,H,T,D)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv=None):\n",
    "        T, B, C = x.shape\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = self._split_heads(q, T, B)\n",
    "        k = self._split_heads(k, T, B)\n",
    "        v = self._split_heads(v, T, B)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            k_cache, v_cache = past_kv\n",
    "            if k_cache is not None:\n",
    "                k = torch.cat([k_cache, k], dim=2)\n",
    "                v = torch.cat([v_cache, v], dim=2)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B,H,T,T_total)\n",
    "        if past_kv is None and T > 1:\n",
    "            causal_mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        ctx = torch.matmul(attn_probs, v)  # (B,H,T,D)\n",
    "        ctx = ctx.permute(2, 0, 1, 3).contiguous().view(T, B, C)\n",
    "        out = self.o_proj(ctx)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            return out, (k, v)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single decoder block: RMSNorm -> CausalAttn (residual) -> RMSNorm -> MLP (residual).\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads)\n",
    "        self.mlp_norm = RMSNorm(d_model)\n",
    "        inner = int(mlp_ratio * d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, inner, bias=False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(inner, d_model, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv=None):\n",
    "        if past_kv is None:\n",
    "            x = x + self.attn(self.attn_norm(x))\n",
    "        else:\n",
    "            attn_out, new_kv = self.attn(self.attn_norm(x), past_kv=past_kv)\n",
    "            x = x + attn_out\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "        if past_kv is not None:\n",
    "            return x, new_kv\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Decoder-only causal Transformer producing logits for next-token prediction.\"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size: int = 50257,\n",
    "                 d_model: int = 512,\n",
    "                 n_heads: int = 8,\n",
    "                 n_blocks: int = 6,\n",
    "                 max_seq_len: int = 2048,\n",
    "                 mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, mlp_ratio) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embed.weight  # Weight tying\n",
    "\n",
    "    def forward(self, tokens_seq: torch.Tensor, kv_cache=None):\n",
    "        T, B = tokens_seq.shape\n",
    "        if T > self.max_seq_len:\n",
    "            tokens_seq = tokens_seq[-self.max_seq_len:]\n",
    "            T = tokens_seq.shape[0]\n",
    "        if kv_cache is not None and len(kv_cache) > 0 and kv_cache[0][0] is not None:\n",
    "            cached_len = kv_cache[0][0].size(2)\n",
    "        else:\n",
    "            cached_len = 0\n",
    "        pos = torch.arange(cached_len, cached_len + T, device=tokens_seq.device)\n",
    "        x = self.token_embed(tokens_seq) + self.pos_embed(pos).unsqueeze(1)\n",
    "        new_cache = [] if kv_cache is not None else None\n",
    "        if kv_cache is None:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "        else:\n",
    "            for blk, past in zip(self.blocks, kv_cache):\n",
    "                x, updated = blk(x, past_kv=past)\n",
    "                new_cache.append(updated)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if kv_cache is not None:\n",
    "            return logits, new_cache\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ac847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(model,\n",
    "                    loader,\n",
    "                    epochs,\n",
    "                    model_name,\n",
    "                    device,\n",
    "                    lr=1e-3,\n",
    "                    log_steps=100,\n",
    "                    sample_interval=30,\n",
    "                    max_steps_per_epoch=None,\n",
    "                    enc=None,\n",
    "                    monosemantic_info=None,\n",
    "                    prompt=\"Once upon a\",\n",
    "                    log_csv_path: str = \"\",\n",
    "                    log_flush_steps: int = 100):\n",
    "    \"\"\"\n",
    "    We add `prompt` as an explicit argument so we can pass it down from main().\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Buffered logging setup\n",
    "    loss_buffer = []\n",
    "    csv_file = None\n",
    "    if log_csv_path:\n",
    "        log_dir = os.path.dirname(log_csv_path)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "        file_exists = os.path.exists(log_csv_path)\n",
    "        csv_file = open(log_csv_path, 'a', newline='')\n",
    "        # Write header if empty\n",
    "        if not file_exists or os.path.getsize(log_csv_path) == 0:\n",
    "            csv_file.write('timestamp,model,epoch,step_in_epoch,global_step,loss\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    next_sample_time = start_time\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        partial_loss = 0.0\n",
    "        partial_count = 0\n",
    "\n",
    "        step_in_epoch = 0\n",
    "        for batch_idx, batch_tokens in enumerate(loader, start=1):\n",
    "            step_in_epoch += 1\n",
    "            global_step += 1\n",
    "\n",
    "            batch_tokens = batch_tokens.to(device)  # (seq_len, batch)\n",
    "\n",
    "            logits = model(batch_tokens)  # (seq_len, batch, vocab_size)\n",
    "            loss = compute_next_token_loss(logits, batch_tokens)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            partial_loss += loss.item()\n",
    "            partial_count += 1\n",
    "\n",
    "            # Buffer this step's loss\n",
    "            if csv_file is not None:\n",
    "                loss_buffer.append(f\"{time.time()},{model_name},{epoch},{step_in_epoch},{global_step},{loss.item()}\\n\")\n",
    "                if len(loss_buffer) >= log_flush_steps:\n",
    "                    csv_file.writelines(loss_buffer)\n",
    "                    csv_file.flush()\n",
    "                    loss_buffer.clear()\n",
    "\n",
    "            if batch_idx % log_steps == 0:\n",
    "                avg_part_loss = partial_loss / partial_count\n",
    "                print(f\"[{model_name}] Epoch {epoch}/{epochs}, \"\n",
    "                      f\"Step {batch_idx}/{len(loader)} (global step: {global_step}) \"\n",
    "                      f\"Partial Avg Loss: {avg_part_loss:.4f}\")\n",
    "                partial_loss = 0.0\n",
    "                partial_count = 0\n",
    "\n",
    "            current_time = time.time()\n",
    "            if current_time >= next_sample_time and enc is not None:\n",
    "                with torch.no_grad():\n",
    "                    print(f\"\\n[{model_name}] Generating sample text (greedy) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_greedy, ann_greedy = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=None,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Greedy Sample: {text_greedy}\")\n",
    "                    print(f\" Annotated: {ann_greedy}\\n\")\n",
    "\n",
    "                    print(f\"[{model_name}] Generating sample text (top-p=0.95) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_topp, ann_topp = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=0.95,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Top-p (p=0.95) Sample: {text_topp}\")\n",
    "                    print(f\" Annotated: {ann_topp}\\n\")\n",
    "\n",
    "                    print(f\"[{model_name}] Generating sample text (top-p=1.0) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_topp1, ann_topp1 = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=1.0,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Top-p (p=1.0) Sample: {text_topp1}\")\n",
    "                    print(f\" Annotated: {ann_topp1}\\n\")\n",
    "\n",
    "                next_sample_time = current_time + sample_interval\n",
    "\n",
    "            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:\n",
    "                print(f\"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, ending epoch {epoch} early.\")\n",
    "                break\n",
    "\n",
    "        avg_loss = total_loss / step_in_epoch\n",
    "        print(f\"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Final flush\n",
    "    if csv_file is not None:\n",
    "        if loss_buffer:\n",
    "            csv_file.writelines(loss_buffer)\n",
    "            csv_file.flush()\n",
    "        csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7a6514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (token_embed): Embedding(50257, 256)\n",
      "  (pos_embed): Embedding(128, 256)\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (attn_norm): RMSNorm()\n",
      "      (attn): CausalSelfAttention(\n",
      "        (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "      )\n",
      "      (mlp_norm): RMSNorm()\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=False)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example: Create and move a model to device\n",
    "# Uncomment the model you want to use and adjust parameters as needed.\n",
    "\n",
    "# K-gram MLP model\n",
    "# model = KGramMLPSeqModel(vocab_size, k=3, embed_size=512, num_inner_layers=1, chunk_size=1)\n",
    "\n",
    "# LSTM model\n",
    "# model = LSTMSeqModel(vocab_size, embed_size=512, hidden_size=512)\n",
    "\n",
    "# Transformer model\n",
    "model = TransformerModel(vocab_size=vocab_size, d_model=256, n_heads=4, n_blocks=2, max_seq_len=block_size)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "\n",
    "# After running this cell, you can use the 'model' variable in training or evaluation cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37aae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, enc, init_text, max_new_tokens=20, device=\"cpu\",\n",
    "                  top_p=None,\n",
    "                  monosemantic_info=None,\n",
    "                  do_monosemantic=False,\n",
    "                  use_kv_cache=False):\n",
    "    \"\"\"\n",
    "    A single code path for all models:\n",
    "      - We keep a growing list 'context_tokens'.\n",
    "      - At each step, we feed the entire context as (seq_len,1) to model(...).\n",
    "      - We get model(...)->(seq_len,1,vocab_size). We take the final step's logits => logits[-1,0,:].\n",
    "      - We pick next token (greedy or top-p), append to context_tokens.\n",
    "      - Optionally do monosemantic analysis on that newly generated token.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        context_tokens = enc.encode(init_text)\n",
    "        annotation_list = []\n",
    "\n",
    "        kv_cache = None\n",
    "        # Prime KV cache step-by-step over the initial prompt for strict causality\n",
    "        if use_kv_cache and hasattr(model, 'blocks') and len(context_tokens) > 0:\n",
    "            kv_cache = [(None, None) for _ in range(len(model.blocks))]\n",
    "            for tid in context_tokens:\n",
    "                tok = torch.tensor([tid], dtype=torch.long, device=device).unsqueeze(1)  # (1,1)\n",
    "                _, kv_cache = model(tok, kv_cache=kv_cache)\n",
    "\n",
    "        for step_i in range(max_new_tokens):\n",
    "            if use_kv_cache and hasattr(model, 'blocks'):\n",
    "                # Use only the last token and advance cache\n",
    "                if len(context_tokens) == 0:\n",
    "                    # Fallback: no context, create a space token as a starter\n",
    "                    last_id = enc.encode(\" \")[-1]\n",
    "                else:\n",
    "                    last_id = context_tokens[-1]\n",
    "                last_token = torch.tensor([last_id], dtype=torch.long, device=device).unsqueeze(1)  # (1,1)\n",
    "                logits_seq, kv_cache = model(last_token, kv_cache=kv_cache if kv_cache is not None else [(None, None) for _ in range(len(model.blocks))])\n",
    "                next_logits = logits_seq[-1, 0, :]\n",
    "            else:\n",
    "                # Fallback: full context each step (works for all models)\n",
    "                seq_tensor = torch.tensor(context_tokens, dtype=torch.long, device=device).unsqueeze(1)\n",
    "                logits_seq = model(seq_tensor)              # (seq_len,1,vocab_size)\n",
    "                next_logits = logits_seq[-1, 0, :]         # shape (vocab_size,)\n",
    "\n",
    "            if top_p is None:\n",
    "                # greedy\n",
    "                chosen_token = torch.argmax(next_logits).item()\n",
    "            else:\n",
    "                chosen_token = nucleus_sampling(next_logits, p=top_p)\n",
    "\n",
    "            context_tokens.append(chosen_token)\n",
    "\n",
    "            if do_monosemantic and monosemantic_info is not None:\n",
    "                neighbors = monosemantic_analysis_for_token(\n",
    "                    chosen_token, model, monosemantic_info, enc, device=device, top_n=5\n",
    "                )\n",
    "                annotation_list.append((chosen_token, neighbors))\n",
    "            else:\n",
    "                annotation_list.append((chosen_token, []))\n",
    "\n",
    "    model.train(was_training)\n",
    "\n",
    "    final_text = enc.decode(context_tokens)\n",
    "    prefix_text = enc.decode(context_tokens[:-max_new_tokens])\n",
    "    annotated_strs = [prefix_text]\n",
    "    for (tid, neighs) in annotation_list:\n",
    "        token_str = enc.decode([tid])\n",
    "        if neighs:\n",
    "            neighbor_strs = [f\"{enc.decode([x[1]])}\" for x in neighs]\n",
    "            annotated = f\"{token_str}[NN={neighbor_strs}]\"\n",
    "        else:\n",
    "            annotated = token_str\n",
    "        annotated_strs.append(annotated)\n",
    "\n",
    "    annotated_text = \"\".join(annotated_strs)\n",
    "    return final_text, annotated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd69d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(logits, p=0.95):\n",
    "    # Convert logits to probabilities\n",
    "    prob_dist = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Sort probabilities in descending order and get indices\n",
    "    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True)\n",
    "\n",
    "    # Compute cumulative sum\n",
    "    cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # Find the cutoff index where cumulative probability exceeds p\n",
    "    # We want to include all tokens up to where cumsum first exceeds p\n",
    "    cutoff_mask = cumsum_probs <= p\n",
    "\n",
    "    # Always include at least the first token (highest probability)\n",
    "    # This handles edge case where first token alone has prob > p\n",
    "    cutoff_mask[0] = True\n",
    "\n",
    "    # Zero out probabilities beyond the nucleus\n",
    "    filtered_probs = sorted_probs.clone()\n",
    "    filtered_probs[~cutoff_mask] = 0.0\n",
    "\n",
    "    # Renormalize the remaining probabilities\n",
    "    filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "\n",
    "    # Sample from the filtered distribution\n",
    "    sampled_index = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "\n",
    "    # Map back to original token index\n",
    "    chosen_token = sorted_indices[sampled_index].item()\n",
    "\n",
    "    return chosen_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f70e0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(model,\n",
    "                    loader,\n",
    "                    epochs,\n",
    "                    model_name,\n",
    "                    device,\n",
    "                    lr=1e-3,\n",
    "                    log_steps=100,\n",
    "                    sample_interval=30,\n",
    "                    max_steps_per_epoch=None,\n",
    "                    enc=None,\n",
    "                    monosemantic_info=None,\n",
    "                    prompt=\"Once upon a\",\n",
    "                    log_csv_path: str = \"\",\n",
    "                    log_flush_steps: int = 100):\n",
    "    \"\"\"\n",
    "    We add `prompt` as an explicit argument so we can pass it down from main().\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Buffered logging setup\n",
    "    loss_buffer = []\n",
    "    csv_file = None\n",
    "    if log_csv_path:\n",
    "        log_dir = os.path.dirname(log_csv_path)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "        file_exists = os.path.exists(log_csv_path)\n",
    "        csv_file = open(log_csv_path, 'a', newline='')\n",
    "        # Write header if empty\n",
    "        if not file_exists or os.path.getsize(log_csv_path) == 0:\n",
    "            csv_file.write('timestamp,model,epoch,step_in_epoch,global_step,loss\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    next_sample_time = start_time\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        partial_loss = 0.0\n",
    "        partial_count = 0\n",
    "\n",
    "        step_in_epoch = 0\n",
    "        for batch_idx, batch_tokens in enumerate(loader, start=1):\n",
    "            step_in_epoch += 1\n",
    "            global_step += 1\n",
    "\n",
    "            batch_tokens = batch_tokens.to(device)  # (seq_len, batch)\n",
    "\n",
    "            logits = model(batch_tokens)  # (seq_len, batch, vocab_size)\n",
    "            loss = compute_next_token_loss(logits, batch_tokens)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            partial_loss += loss.item()\n",
    "            partial_count += 1\n",
    "\n",
    "            # Buffer this step's loss\n",
    "            if csv_file is not None:\n",
    "                loss_buffer.append(f\"{time.time()},{model_name},{epoch},{step_in_epoch},{global_step},{loss.item()}\\n\")\n",
    "                if len(loss_buffer) >= log_flush_steps:\n",
    "                    csv_file.writelines(loss_buffer)\n",
    "                    csv_file.flush()\n",
    "                    loss_buffer.clear()\n",
    "\n",
    "            if batch_idx % log_steps == 0:\n",
    "                avg_part_loss = partial_loss / partial_count\n",
    "                print(f\"[{model_name}] Epoch {epoch}/{epochs}, \"\n",
    "                      f\"Step {batch_idx}/{len(loader)} (global step: {global_step}) \"\n",
    "                      f\"Partial Avg Loss: {avg_part_loss:.4f}\")\n",
    "                partial_loss = 0.0\n",
    "                partial_count = 0\n",
    "\n",
    "            current_time = time.time()\n",
    "            if current_time >= next_sample_time and enc is not None:\n",
    "                with torch.no_grad():\n",
    "                    print(f\"\\n[{model_name}] Generating sample text (greedy) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_greedy, ann_greedy = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=None,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Greedy Sample: {text_greedy}\")\n",
    "                    print(f\" Annotated: {ann_greedy}\\n\")\n",
    "\n",
    "                    print(f\"[{model_name}] Generating sample text (top-p=0.95) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_topp, ann_topp = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=0.95,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Top-p (p=0.95) Sample: {text_topp}\")\n",
    "                    print(f\" Annotated: {ann_topp}\\n\")\n",
    "\n",
    "                    print(f\"[{model_name}] Generating sample text (top-p=1.0) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_topp1, ann_topp1 = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=1.0,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Top-p (p=1.0) Sample: {text_topp1}\")\n",
    "                    print(f\" Annotated: {ann_topp1}\\n\")\n",
    "\n",
    "                next_sample_time = current_time + sample_interval\n",
    "\n",
    "            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:\n",
    "                print(f\"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, ending epoch {epoch} early.\")\n",
    "                break\n",
    "\n",
    "        avg_loss = total_loss / step_in_epoch\n",
    "        print(f\"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Final flush\n",
    "    if csv_file is not None:\n",
    "        if loss_buffer:\n",
    "            csv_file.writelines(loss_buffer)\n",
    "            csv_file.flush()\n",
    "        csv_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfd5c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1...\n",
      " Greedy Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...\n",
      " Greedy Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...\n",
      " Top-p (p=0.95) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1...\n",
      " Top-p (p=0.95) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1...\n",
      " Top-p (p=1.0) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=62...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=62...\n",
      " Greedy Sample: Once upon a time a.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Annotated: Once upon a time a.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=62...\n",
      " Greedy Sample: Once upon a time a.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Annotated: Once upon a time a.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=62...\n",
      " Top-p (p=0.95) Sample: Once upon a time there there there there sarcastic sarcastic sarcastic having a a a air was delighted delighted delighted passengers passengers. vascular\n",
      " Annotated: Once upon a time there there there there sarcastic sarcastic sarcastic having a a a air was delighted delighted delighted passengers passengers. vascular\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=62...\n",
      " Top-p (p=0.95) Sample: Once upon a time there there there there sarcastic sarcastic sarcastic having a a a air was delighted delighted delighted passengers passengers. vascular\n",
      " Annotated: Once upon a time there there there there sarcastic sarcastic sarcastic having a a a air was delighted delighted delighted passengers passengers. vascular\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=62...\n",
      " Top-p (p=1.0) Sample: Once upon a time a her registers aka. fluor fluor there was Prohibition her her aNs a Bucket Bucket Bucket Bucket.\n",
      " Annotated: Once upon a time a her registers aka. fluor fluor there was Prohibition her her aNs a Bucket Bucket Bucket Bucket.\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time a her registers aka. fluor fluor there was Prohibition her her aNs a Bucket Bucket Bucket Bucket.\n",
      " Annotated: Once upon a time a her registers aka. fluor fluor there was Prohibition her her aNs a Bucket Bucket Bucket Bucket.\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 100/1250 (global step: 100) Partial Avg Loss: 25.4268\n",
      "[Transformer] Epoch 1/2, Step 100/1250 (global step: 100) Partial Avg Loss: 25.4268\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=126...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=126...\n",
      " Greedy Sample: Once upon a time, there was a little girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl\n",
      " Annotated: Once upon a time, there was a little girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=126...\n",
      " Greedy Sample: Once upon a time, there was a little girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl\n",
      " Annotated: Once upon a time, there was a little girl girl girl girl girl girl girl girl girl girl girl girl girl girl girl\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=126...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was coldCan to. jam the elimination.Yes and could saw the matter dad see treacherous treacherous\n",
      " Annotated: Once upon a time there was coldCan to. jam the elimination.Yes and could saw the matter dad see treacherous treacherous\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=126...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was coldCan to. jam the elimination.Yes and could saw the matter dad see treacherous treacherous\n",
      " Annotated: Once upon a time there was coldCan to. jam the elimination.Yes and could saw the matter dad see treacherous treacherous\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=126...\n",
      " Top-p (p=1.0) Sample: Once upon a time there day to treacherous treacherous what Tim. One was happy oyellereller a. her her happened happened\n",
      " Annotated: Once upon a time there day to treacherous treacherous what Tim. One was happy oyellereller a. her her happened happened\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there day to treacherous treacherous what Tim. One was happy oyellereller a. her her happened happened\n",
      " Annotated: Once upon a time there day to treacherous treacherous what Tim. One was happy oyellereller a. her her happened happened\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=187...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=187...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She. She was very very. She was so so\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She. She was very very. She was so so\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=187...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She. She was very very. She was so so\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She. She was very very. She was so so\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=187...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a Sara, outside M. It wasL her Lily. He what. They down\n",
      " Annotated: Once upon a time, there was a Sara, outside M. It wasL her Lily. He what. They down\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=187...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a Sara, outside M. It wasL her Lily. He what. They down\n",
      " Annotated: Once upon a time, there was a Sara, outside M. It wasL her Lily. He what. They down\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=187...\n",
      " Top-p (p=1.0) Sample: Once upon a time,, there celebrating mommy sad find find, her morning of a cream phenotype and said, outside\n",
      " Annotated: Once upon a time,, there celebrating mommy sad find find, her morning of a cream phenotype and said, outside\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time,, there celebrating mommy sad find find, her morning of a cream phenotype and said, outside\n",
      " Annotated: Once upon a time,, there celebrating mommy sad find find, her morning of a cream phenotype and said, outside\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 200/1250 (global step: 200) Partial Avg Loss: 8.2184\n",
      "[Transformer] Epoch 1/2, Step 200/1250 (global step: 200) Partial Avg Loss: 8.2184\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=255...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her. One day, but\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her. One day, but\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=255...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=255...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her. One day, but\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her. One day, but\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=255...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there time in the named holding pirates. They far going He getting the't. dragon!!\n",
      " Annotated: Once upon a time, there time in the named holding pirates. They far going He getting the't. dragon!!\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=255...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there time in the named holding pirates. They far going He getting the't. dragon!!\n",
      " Annotated: Once upon a time, there time in the named holding pirates. They far going He getting the't. dragon!!\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=255...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a Timmy tree and to for swing it!\" Timmy. \n",
      " night noise\n",
      " Annotated: Once upon a time, there was a Timmy tree and to for swing it!\" Timmy. \n",
      " night noise\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a Timmy tree and to for swing it!\" Timmy. \n",
      " night noise\n",
      " Annotated: Once upon a time, there was a Timmy tree and to for swing it!\" Timmy. \n",
      " night noise\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 300/1250 (global step: 300) Partial Avg Loss: 6.0237\n",
      "[Transformer] Epoch 1/2, Step 300/1250 (global step: 300) Partial Avg Loss: 6.0237\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=325...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her mom was so so she\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her mom was so so she\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=325...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=325...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her mom was so so she\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her mom was so so she\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=325...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little that loved to two toys everything was very in a garden..\" girl named\n",
      " Annotated: Once upon a time, there was a little that loved to two toys everything was very in a garden..\" girl named\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=325...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little that loved to two toys everything was very in a garden..\" girl named\n",
      " Annotated: Once upon a time, there was a little that loved to two toys everything was very in a garden..\" girl named\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=325...\n",
      " Top-p (p=1.0) Sample: Once upon a time a time sticks very who finger!\" that he wanted tophis sea over adventure a big out that they\n",
      " Annotated: Once upon a time a time sticks very who finger!\" that he wanted tophis sea over adventure a big out that they\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time a time sticks very who finger!\" that he wanted tophis sea over adventure a big out that they\n",
      " Annotated: Once upon a time a time sticks very who finger!\" that he wanted tophis sea over adventure a big out that they\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=396...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with his friends. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with his friends. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=396...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=396...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with his friends. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with his friends. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=396...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. He continued mum loved to his boy his Lily was many\n",
      " Annotated: Once upon a time, there was a little girl named Lily. He continued mum loved to his boy his Lily was many\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=396...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. He continued mum loved to his boy his Lily was many\n",
      " Annotated: Once upon a time, there was a little girl named Lily. He continued mum loved to his boy his Lily was many\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=396...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a have her charming in the hot smiled in an,!\" Lily looked said owner glass\n",
      " Annotated: Once upon a time, there was a have her charming in the hot smiled in an,!\" Lily looked said owner glass\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a have her charming in the hot smiled in an,!\" Lily looked said owner glass\n",
      " Annotated: Once upon a time, there was a have her charming in the hot smiled in an,!\" Lily looked said owner glass\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 400/1250 (global step: 400) Partial Avg Loss: 5.3902\n",
      "[Transformer] Epoch 1/2, Step 400/1250 (global step: 400) Partial Avg Loss: 5.3902\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=467...\n",
      " Greedy Sample: Once upon a time there was a little girl named Lily. She loved to play with her mommy loved to play with\n",
      " Annotated: Once upon a time there was a little girl named Lily. She loved to play with her mommy loved to play with\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=467...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=467...\n",
      " Greedy Sample: Once upon a time there was a little girl named Lily. She loved to play with her mommy loved to play with\n",
      " Annotated: Once upon a time there was a little girl named Lily. She loved to play with her mommy loved to play with\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=467...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a big middle of \" arms. She was very high and ran. One day all Tim\n",
      " Annotated: Once upon a time there was a big middle of \" arms. She was very high and ran. One day all Tim\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=467...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a big middle of \" arms. She was very high and ran. One day all Tim\n",
      " Annotated: Once upon a time there was a big middle of \" arms. She was very high and ran. One day all Tim\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=467...\n",
      " Top-p (p=1.0) Sample: Once upon a time there were joy!\" still stilly.\n",
      "\n",
      " stick on a little girl, she day he living\n",
      " Annotated: Once upon a time there were joy!\" still stilly.\n",
      "\n",
      " stick on a little girl, she day he living\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there were joy!\" still stilly.\n",
      "\n",
      " stick on a little girl, she day he living\n",
      " Annotated: Once upon a time there were joy!\" still stilly.\n",
      "\n",
      " stick on a little girl, she day he living\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 500/1250 (global step: 500) Partial Avg Loss: 5.0721\n",
      "[Transformer] Epoch 1/2, Step 500/1250 (global step: 500) Partial Avg Loss: 5.0721\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=539...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her mommy. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her mommy. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=539...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=539...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her mommy. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her mommy. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=539...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a little girl named Lily was dad was ran girl,\" came to don't know. One\n",
      " Annotated: Once upon a time there was a little girl named Lily was dad was ran girl,\" came to don't know. One\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=539...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a little girl named Lily was dad was ran girl,\" came to don't know. One\n",
      " Annotated: Once upon a time there was a little girl named Lily was dad was ran girl,\" came to don't know. One\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=539...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little pretty named Lily. So. One day not want record fell oct without.\n",
      " Annotated: Once upon a time, there was a little pretty named Lily. So. One day not want record fell oct without.\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little pretty named Lily. So. One day not want record fell oct without.\n",
      " Annotated: Once upon a time, there was a little pretty named Lily. So. One day not want record fell oct without.\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 600/1250 (global step: 600) Partial Avg Loss: 4.8418\n",
      "[Transformer] Epoch 1/2, Step 600/1250 (global step: 600) Partial Avg Loss: 4.8418\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=610...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside. One day, she had\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside. One day, she had\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=610...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=610...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside. One day, she had\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside. One day, she had\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=610...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little boy wanted. So always old, Timmy went outside. He was searched\n",
      " Annotated: Once upon a time, there was a little boy wanted. So always old, Timmy went outside. He was searched\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=610...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little boy wanted. So always old, Timmy went outside. He was searched\n",
      " Annotated: Once upon a time, there was a little boy wanted. So always old, Timmy went outside. He was searched\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=610...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a hole day. They like door and rolled enough birthday in feeling then, and soft\n",
      " Annotated: Once upon a time, there was a hole day. They like door and rolled enough birthday in feeling then, and soft\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a hole day. They like door and rolled enough birthday in feeling then, and soft\n",
      " Annotated: Once upon a time, there was a hole day. They like door and rolled enough birthday in feeling then, and soft\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=681...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her mom and\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her mom and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=681...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=681...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her mom and\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her mom and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=681...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a small named She was 3 shouted and saw a big look. One day he mustache\n",
      " Annotated: Once upon a time, there was a small named She was 3 shouted and saw a big look. One day he mustache\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=681...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a small named She was 3 shouted and saw a big look. One day he mustache\n",
      " Annotated: Once upon a time, there was a small named She was 3 shouted and saw a big look. One day he mustache\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=681...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there lived there was a running running years very farmerWhere of blue '' man lots ofs listened\n",
      " Annotated: Once upon a time, there lived there was a running running years very farmerWhere of blue '' man lots ofs listened\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there lived there was a running running years very farmerWhere of blue '' man lots ofs listened\n",
      " Annotated: Once upon a time, there lived there was a running running years very farmerWhere of blue '' man lots ofs listened\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 700/1250 (global step: 700) Partial Avg Loss: 4.7057\n",
      "[Transformer] Epoch 1/2, Step 700/1250 (global step: 700) Partial Avg Loss: 4.7057\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=751...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to help her mommy. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to help her mommy. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=751...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=751...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to help her mommy. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to help her mommy. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=751...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a girl named She was quickly. \n",
      "\n",
      "Her of his mom took the on a\n",
      " Annotated: Once upon a time there was a girl named She was quickly. \n",
      "\n",
      "Her of his mom took the on a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=751...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a girl named She was quickly. \n",
      "\n",
      "Her of his mom took the on a\n",
      " Annotated: Once upon a time there was a girl named She was quickly. \n",
      "\n",
      "Her of his mom took the on a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=751...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a anymore.\n",
      "But. worked in a came lived with the giving about him you dark\n",
      " Annotated: Once upon a time there was a anymore.\n",
      "But. worked in a came lived with the giving about him you dark\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a anymore.\n",
      "But. worked in a came lived with the giving about him you dark\n",
      " Annotated: Once upon a time there was a anymore.\n",
      "But. worked in a came lived with the giving about him you dark\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 800/1250 (global step: 800) Partial Avg Loss: 4.5837\n",
      "[Transformer] Epoch 1/2, Step 800/1250 (global step: 800) Partial Avg Loss: 4.5837\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=822...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she was very\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she was very\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=822...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=822...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she was very\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she was very\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=822...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a bird. One was so happy andDo smiled and safe flowers with now drank left\n",
      " Annotated: Once upon a time, there was a bird. One was so happy andDo smiled and safe flowers with now drank left\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=822...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a bird. One was so happy andDo smiled and safe flowers with now drank left\n",
      " Annotated: Once upon a time, there was a bird. One was so happy andDo smiled and safe flowers with now drank left\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=822...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl each who pointed. She jumped.\n",
      "\n",
      "One day she wanted to\n",
      " Annotated: Once upon a time, there was a little girl each who pointed. She jumped.\n",
      "\n",
      "One day she wanted to\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl each who pointed. She jumped.\n",
      "\n",
      "One day she wanted to\n",
      " Annotated: Once upon a time, there was a little girl each who pointed. She jumped.\n",
      "\n",
      "One day she wanted to\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=893...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=893...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=893...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=893...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a rain a only only explained. One day he wanted a big toy. Tom loved\n",
      " Annotated: Once upon a time, there was a rain a only only explained. One day he wanted a big toy. Tom loved\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=893...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a rain a only only explained. One day he wanted a big toy. Tom loved\n",
      " Annotated: Once upon a time, there was a rain a only only explained. One day he wanted a big toy. Tom loved\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=893...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a boy named Tom. He wanted to the sign. One day, he place lay\n",
      " Annotated: Once upon a time, there was a boy named Tom. He wanted to the sign. One day, he place lay\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a boy named Tom. He wanted to the sign. One day, he place lay\n",
      " Annotated: Once upon a time, there was a boy named Tom. He wanted to the sign. One day, he place lay\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 900/1250 (global step: 900) Partial Avg Loss: 4.4980\n",
      "[Transformer] Epoch 1/2, Step 900/1250 (global step: 900) Partial Avg Loss: 4.4980\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=964...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big and\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=964...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=964...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big and\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=964...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a what girl named Lily. One day, she loved to play in the burning to\n",
      " Annotated: Once upon a time, there was a what girl named Lily. One day, she loved to play in the burning to\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=964...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a what girl named Lily. One day, she loved to play in the burning to\n",
      " Annotated: Once upon a time, there was a what girl named Lily. One day, she loved to play in the burning to\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=964...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a marble balloon. It was soup mom asked he wanted to school, when he\n",
      " Annotated: Once upon a time, there was a marble balloon. It was soup mom asked he wanted to school, when he\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a marble balloon. It was soup mom asked he wanted to school, when he\n",
      " Annotated: Once upon a time, there was a marble balloon. It was soup mom asked he wanted to school, when he\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 1000/1250 (global step: 1000) Partial Avg Loss: 4.3914\n",
      "[Transformer] Epoch 1/2, Step 1000/1250 (global step: 1000) Partial Avg Loss: 4.3914\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1035...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and a big,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and a big,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1035...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1035...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and a big,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and a big,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1035...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She had a big thanked and join some oct park and\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She had a big thanked and join some oct park and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1035...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She had a big thanked and join some oct park and\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She had a big thanked and join some oct park and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1035...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a little boy named He followed proud. Timmy wanted to play with his mommy years\n",
      " Annotated: Once upon a time there was a little boy named He followed proud. Timmy wanted to play with his mommy years\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a little boy named He followed proud. Timmy wanted to play with his mommy years\n",
      " Annotated: Once upon a time there was a little boy named He followed proud. Timmy wanted to play with his mommy years\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 1100/1250 (global step: 1100) Partial Avg Loss: 4.3053\n",
      "[Transformer] Epoch 1/2, Step 1100/1250 (global step: 1100) Partial Avg Loss: 4.3053\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1106...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1106...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1106...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and she had a big,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1106...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to play outside with her friends. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside with her friends. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1106...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to play outside with her friends. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside with her friends. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1106...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. Lily loved to play things, wait wait tooc.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. Lily loved to play things, wait wait tooc.\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. Lily loved to play things, wait wait tooc.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. Lily loved to play things, wait wait tooc.\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1177...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she saw a\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she saw a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1177...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1177...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she saw a\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and she saw a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1177...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named hunter called shop. She wass house with her very new of\n",
      " Annotated: Once upon a time, there was a little girl named hunter called shop. She wass house with her very new of\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1177...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named hunter called shop. She wass house with her very new of\n",
      " Annotated: Once upon a time, there was a little girl named hunter called shop. She wass house with her very new of\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1177...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a young beautiful Bobopus, there was three years old and he spinning so excited to clean\n",
      " Annotated: Once upon a time there was a young beautiful Bobopus, there was three years old and he spinning so excited to clean\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a young beautiful Bobopus, there was three years old and he spinning so excited to clean\n",
      " Annotated: Once upon a time there was a young beautiful Bobopus, there was three years old and he spinning so excited to clean\n",
      "\n",
      "[Transformer] Epoch 1/2, Step 1200/1250 (global step: 1200) Partial Avg Loss: 4.2347\n",
      "[Transformer] Epoch 1/2, Step 1200/1250 (global step: 1200) Partial Avg Loss: 4.2347\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1248...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1248...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1248...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1248...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. One day, Lily left the trees trees. She ran\n",
      " Annotated: Once upon a time, there was a little girl named Lily. One day, Lily left the trees trees. She ran\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1248...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. One day, Lily left the trees trees. She ran\n",
      " Annotated: Once upon a time, there was a little girl named Lily. One day, Lily left the trees trees. She ran\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1248...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a little boy called Timmy. They saw many beach in the sky. His house in\n",
      " Annotated: Once upon a time there was a little boy called Timmy. They saw many beach in the sky. His house in\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a little boy called Timmy. They saw many beach in the sky. His house in\n",
      " Annotated: Once upon a time there was a little boy called Timmy. They saw many beach in the sky. His house in\n",
      "\n",
      "[Transformer] *** End of Epoch 1 *** Avg Loss: 6.7031\n",
      "[Transformer] *** End of Epoch 1 *** Avg Loss: 6.7031\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=69...\n",
      " Greedy Sample: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      " Annotated: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=69...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=69...\n",
      " Greedy Sample: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      " Annotated: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=69...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little boy named Tim. He loved toHe big sandwich near with his friends.\n",
      " Annotated: Once upon a time, there was a little boy named Tim. He loved toHe big sandwich near with his friends.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=69...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little boy named Tim. He loved toHe big sandwich near with his friends.\n",
      " Annotated: Once upon a time, there was a little boy named Tim. He loved toHe big sandwich near with his friends.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=69...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a than wonderful red'll the frog. The noise items particular day, his toy person\n",
      " Annotated: Once upon a time, there was a than wonderful red'll the frog. The noise items particular day, his toy person\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a than wonderful red'll the frog. The noise items particular day, his toy person\n",
      " Annotated: Once upon a time, there was a than wonderful red'll the frog. The noise items particular day, his toy person\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 100/1250 (global step: 1350) Partial Avg Loss: 4.1643\n",
      "[Transformer] Epoch 2/2, Step 100/1250 (global step: 1350) Partial Avg Loss: 4.1643\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=140...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=140...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=140...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=140...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a forest andTim. He was four and joy outside, he had an old and\n",
      " Annotated: Once upon a time, there was a forest andTim. He was four and joy outside, he had an old and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=140...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a forest andTim. He was four and joy outside, he had an old and\n",
      " Annotated: Once upon a time, there was a forest andTim. He was four and joy outside, he had an old and\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=140...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a three years old animals. He had riding. He would shop, and loved the\n",
      " Annotated: Once upon a time, there was a three years old animals. He had riding. He would shop, and loved the\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a three years old animals. He had riding. He would shop, and loved the\n",
      " Annotated: Once upon a time, there was a three years old animals. He had riding. He would shop, and loved the\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 200/1250 (global step: 1450) Partial Avg Loss: 4.1021\n",
      "[Transformer] Epoch 2/2, Step 200/1250 (global step: 1450) Partial Avg Loss: 4.1021\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=211...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and made her mom\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and made her mom\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=211...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=211...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and made her mom\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and made her mom\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=211...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to very hard and saw a big owl.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to very hard and saw a big owl.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=211...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to very hard and saw a big owl.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to very hard and saw a big owl.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=211...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She loved to watch accidentally accidentally accidentally no, and create\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to watch accidentally accidentally accidentally no, and create\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She loved to watch accidentally accidentally accidentally no, and create\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to watch accidentally accidentally accidentally no, and create\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=283...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=283...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=283...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=283...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She had a pet no with joy line. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She had a pet no with joy line. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=283...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She had a pet no with joy line. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She had a pet no with joy line. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=283...\n",
      " Top-p (p=1.0) Sample: Once upon a time there lived inisy cell that boat. He loved to find high. It was determined it mean saw\n",
      " Annotated: Once upon a time there lived inisy cell that boat. He loved to find high. It was determined it mean saw\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there lived inisy cell that boat. He loved to find high. It was determined it mean saw\n",
      " Annotated: Once upon a time there lived inisy cell that boat. He loved to find high. It was determined it mean saw\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 300/1250 (global step: 1550) Partial Avg Loss: 4.0665\n",
      "[Transformer] Epoch 2/2, Step 300/1250 (global step: 1550) Partial Avg Loss: 4.0665\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=353...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=353...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=353...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=353...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to play on her ball. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play on her ball. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=353...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to play on her ball. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play on her ball. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=353...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the storeling Jenny.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the storeling Jenny.\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the storeling Jenny.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the storeling Jenny.\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 400/1250 (global step: 1650) Partial Avg Loss: 4.0324\n",
      "[Transformer] Epoch 2/2, Step 400/1250 (global step: 1650) Partial Avg Loss: 4.0324\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=425...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=425...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=425...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=425...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a have fishing on a big castle. Every morning bear and it also usedie on\n",
      " Annotated: Once upon a time, there was a have fishing on a big castle. Every morning bear and it also usedie on\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=425...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a have fishing on a big castle. Every morning bear and it also usedie on\n",
      " Annotated: Once upon a time, there was a have fishing on a big castle. Every morning bear and it also usedie on\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=425...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a big team in the sea. It was a big thick agreed at the slim.\n",
      " Annotated: Once upon a time, there was a big team in the sea. It was a big thick agreed at the slim.\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a big team in the sea. It was a big thick agreed at the slim.\n",
      " Annotated: Once upon a time, there was a big team in the sea. It was a big thick agreed at the slim.\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=496...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=496...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=496...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her mom\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=496...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. Lily was playing with her hands. She lived in with\n",
      " Annotated: Once upon a time, there was a little girl named Lily. Lily was playing with her hands. She lived in with\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=496...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. Lily was playing with her hands. She lived in with\n",
      " Annotated: Once upon a time, there was a little girl named Lily. Lily was playing with her hands. She lived in with\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=496...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a little girl named Sue was playing one. soft.\" Sue was comet and play with her\n",
      " Annotated: Once upon a time there was a little girl named Sue was playing one. soft.\" Sue was comet and play with her\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a little girl named Sue was playing one. soft.\" Sue was comet and play with her\n",
      " Annotated: Once upon a time there was a little girl named Sue was playing one. soft.\" Sue was comet and play with her\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 500/1250 (global step: 1750) Partial Avg Loss: 3.9579\n",
      "[Transformer] Epoch 2/2, Step 500/1250 (global step: 1750) Partial Avg Loss: 3.9579\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=566...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=566...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her friends.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her friends.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=566...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her friends.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside and play with her friends.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=566...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to play outside with her toys. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside with her toys. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=566...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to play outside with her toys. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside with her toys. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=566...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was aBang went. The bearens had a hill needed to play outside and of the\n",
      " Annotated: Once upon a time, there was aBang went. The bearens had a hill needed to play outside and of the\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was aBang went. The bearens had a hill needed to play outside and of the\n",
      " Annotated: Once upon a time, there was aBang went. The bearens had a hill needed to play outside and of the\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 600/1250 (global step: 1850) Partial Avg Loss: 3.9274\n",
      "[Transformer] Epoch 2/2, Step 600/1250 (global step: 1850) Partial Avg Loss: 3.9274\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=636...\n",
      " Greedy Sample: Once upon a time there was a little girl named Lily. She loved to play with her mom and she had a big\n",
      " Annotated: Once upon a time there was a little girl named Lily. She loved to play with her mom and she had a big\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=636...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=636...\n",
      " Greedy Sample: Once upon a time there was a little girl named Lily. She loved to play with her mom and she had a big\n",
      " Annotated: Once upon a time there was a little girl named Lily. She loved to play with her mom and she had a big\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=636...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a boy named Jack books. He wanted to keeper feel friend he could there was a big\n",
      " Annotated: Once upon a time there was a boy named Jack books. He wanted to keeper feel friend he could there was a big\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=636...\n",
      " Top-p (p=0.95) Sample: Once upon a time there was a boy named Jack books. He wanted to keeper feel friend he could there was a big\n",
      " Annotated: Once upon a time there was a boy named Jack books. He wanted to keeper feel friend he could there was a big\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=636...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a small pileopard there lived with a spicy people. But it was ready and how out\n",
      " Annotated: Once upon a time there was a small pileopard there lived with a spicy people. But it was ready and how out\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a small pileopard there lived with a spicy people. But it was ready and how out\n",
      " Annotated: Once upon a time there was a small pileopard there lived with a spicy people. But it was ready and how out\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 700/1250 (global step: 1950) Partial Avg Loss: 3.8797\n",
      "[Transformer] Epoch 2/2, Step 700/1250 (global step: 1950) Partial Avg Loss: 3.8797\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=708...\n",
      " Greedy Sample: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      " Annotated: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=708...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=708...\n",
      " Greedy Sample: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      " Annotated: Once upon a time, there was a little boy named Timmy. Timmy loved to play with his friends. One\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=708...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a creativeide. It was very special scissors stronger with her fish had not to tastes\n",
      " Annotated: Once upon a time, there was a creativeide. It was very special scissors stronger with her fish had not to tastes\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=708...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a creativeide. It was very special scissors stronger with her fish had not to tastes\n",
      " Annotated: Once upon a time, there was a creativeide. It was very special scissors stronger with her fish had not to tastes\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=708...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named pray. One day, Molly saw a film was excitement and spaghetti\n",
      " Annotated: Once upon a time, there was a little girl named pray. One day, Molly saw a film was excitement and spaghetti\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named pray. One day, Molly saw a film was excitement and spaghetti\n",
      " Annotated: Once upon a time, there was a little girl named pray. One day, Molly saw a film was excitement and spaghetti\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=779...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=779...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=779...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=779...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a boy named Jack and always. Timmy loved to first his family. He's\n",
      " Annotated: Once upon a time, there was a boy named Jack and always. Timmy loved to first his family. He's\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=779...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a boy named Jack and always. Timmy loved to first his family. He's\n",
      " Annotated: Once upon a time, there was a boy named Jack and always. Timmy loved to first his family. He's\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=779...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a handle who accidentally grill. He was a wave family. He would separates a three\n",
      " Annotated: Once upon a time, there was a handle who accidentally grill. He was a wave family. He would separates a three\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a handle who accidentally grill. He was a wave family. He would separates a three\n",
      " Annotated: Once upon a time, there was a handle who accidentally grill. He was a wave family. He would separates a three\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 800/1250 (global step: 2050) Partial Avg Loss: 3.8392\n",
      "[Transformer] Epoch 2/2, Step 800/1250 (global step: 2050) Partial Avg Loss: 3.8392\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=850...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=850...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=850...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=850...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little Mom named Sue. Sue was walking and was very excited, and he nodded\n",
      " Annotated: Once upon a time, there was a little Mom named Sue. Sue was walking and was very excited, and he nodded\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=850...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little Mom named Sue. Sue was walking and was very excited, and he nodded\n",
      " Annotated: Once upon a time, there was a little Mom named Sue. Sue was walking and was very excited, and he nodded\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=850...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Sarah. Lucy loved to dance, Daisy in a exciting every day\n",
      " Annotated: Once upon a time, there was a little girl named Sarah. Lucy loved to dance, Daisy in a exciting every day\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Sarah. Lucy loved to dance, Daisy in a exciting every day\n",
      " Annotated: Once upon a time, there was a little girl named Sarah. Lucy loved to dance, Daisy in a exciting every day\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 900/1250 (global step: 2150) Partial Avg Loss: 3.8052\n",
      "[Transformer] Epoch 2/2, Step 900/1250 (global step: 2150) Partial Avg Loss: 3.8052\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=908...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=908...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=908...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=908...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a house, wrote twoed. Every day, the strange appeared at the special bird\n",
      " Annotated: Once upon a time, there was a house, wrote twoed. Every day, the strange appeared at the special bird\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=908...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a house, wrote twoed. Every day, the strange appeared at the special bird\n",
      " Annotated: Once upon a time, there was a house, wrote twoed. Every day, the strange appeared at the special bird\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=908...\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a paint melt. The classroom made drawing's apple tried to explore.\n",
      "\n",
      "One day\n",
      " Annotated: Once upon a time there was a paint melt. The classroom made drawing's apple tried to explore.\n",
      "\n",
      "One day\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time there was a paint melt. The classroom made drawing's apple tried to explore.\n",
      "\n",
      "One day\n",
      " Annotated: Once upon a time there was a paint melt. The classroom made drawing's apple tried to explore.\n",
      "\n",
      "One day\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=966...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the sky. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the sky. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=966...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=966...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the sky. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the sky. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=966...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a mom. Spot saw a shiny place lucky walking through theie in a big toys\n",
      " Annotated: Once upon a time, there was a mom. Spot saw a shiny place lucky walking through theie in a big toys\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=966...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a mom. Spot saw a shiny place lucky walking through theie in a big toys\n",
      " Annotated: Once upon a time, there was a mom. Spot saw a shiny place lucky walking through theie in a big toys\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=966...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She loved to explore pages and hurt you. Every day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to explore pages and hurt you. Every day\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She loved to explore pages and hurt you. Every day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to explore pages and hurt you. Every day\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 1000/1250 (global step: 2250) Partial Avg Loss: 3.7651\n",
      "[Transformer] Epoch 2/2, Step 1000/1250 (global step: 2250) Partial Avg Loss: 3.7651\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1030...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1030...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1030...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1030...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little bird named Lucy. softly pizza was very red. \n",
      "\n",
      "One day\n",
      " Annotated: Once upon a time, there was a little bird named Lucy. softly pizza was very red. \n",
      "\n",
      "One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1030...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little bird named Lucy. softly pizza was very red. \n",
      "\n",
      "One day\n",
      " Annotated: Once upon a time, there was a little bird named Lucy. softly pizza was very red. \n",
      "\n",
      "One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1030...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She had a dog, looking at the toys, a\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She had a dog, looking at the toys, a\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Lily. She had a dog, looking at the toys, a\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She had a dog, looking at the toys, a\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1097...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1097...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1097...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1097...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a garage girl named Lily. Lily loved to build Fin to play with her toy cars\n",
      " Annotated: Once upon a time, there was a garage girl named Lily. Lily loved to build Fin to play with her toy cars\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1097...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a garage girl named Lily. Lily loved to build Fin to play with her toy cars\n",
      " Annotated: Once upon a time, there was a garage girl named Lily. Lily loved to build Fin to play with her toy cars\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1097...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Alexa. They liked to play fairy things in the park. One\n",
      " Annotated: Once upon a time, there was a little girl named Alexa. They liked to play fairy things in the park. One\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little girl named Alexa. They liked to play fairy things in the park. One\n",
      " Annotated: Once upon a time, there was a little girl named Alexa. They liked to play fairy things in the park. One\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 1100/1250 (global step: 2350) Partial Avg Loss: 3.7346\n",
      "[Transformer] Epoch 2/2, Step 1100/1250 (global step: 2350) Partial Avg Loss: 3.7346\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1162...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1162...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1162...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1162...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to go to the park with her mom.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to go to the park with her mom.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1162...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. She loved to go to the park with her mom.\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to go to the park with her mom.\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1162...\n",
      " Top-p (p=1.0) Sample: Once upon a time, in a small kitchen woods. She was very everywhere and curious. The one was excited to happen\n",
      " Annotated: Once upon a time, in a small kitchen woods. She was very everywhere and curious. The one was excited to happen\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, in a small kitchen woods. She was very everywhere and curious. The one was excited to happen\n",
      " Annotated: Once upon a time, in a small kitchen woods. She was very everywhere and curious. The one was excited to happen\n",
      "\n",
      "[Transformer] Epoch 2/2, Step 1200/1250 (global step: 2450) Partial Avg Loss: 3.7041\n",
      "[Transformer] Epoch 2/2, Step 1200/1250 (global step: 2450) Partial Avg Loss: 3.7041\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1218...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=2, step=1218...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play in the park. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play in the park. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1218...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play in the park. One day,\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She loved to play in the park. One day,\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=2, step=1218...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. Sue was very yummy, and Jane got home of\n",
      " Annotated: Once upon a time, there was a little girl named Lily. Sue was very yummy, and Jane got home of\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1218...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little girl named Lily. Sue was very yummy, and Jane got home of\n",
      " Annotated: Once upon a time, there was a little girl named Lily. Sue was very yummy, and Jane got home of\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=2, step=1218...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little jar on the sun. The kiss loved to pour the pit little old man\n",
      " Annotated: Once upon a time, there was a little jar on the sun. The kiss loved to pour the pit little old man\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little jar on the sun. The kiss loved to pour the pit little old man\n",
      " Annotated: Once upon a time, there was a little jar on the sun. The kiss loved to pour the pit little old man\n",
      "\n",
      "[Transformer] *** End of Epoch 2 *** Avg Loss: 3.9051\n",
      "[Transformer] *** End of Epoch 2 *** Avg Loss: 3.9051\n"
     ]
    }
   ],
   "source": [
    "# Train the Transformer model using train_one_model\n",
    "train_one_model(\n",
    "    model=model,                # your TransformerModel instance\n",
    "    loader=train_loader,        # your DataLoader\n",
    "    epochs=epochs,              # number of epochs (from hyperparameters)\n",
    "    model_name=\"Transformer\",   # name for logging\n",
    "    device=device,              # torch.device (CPU or MPS)\n",
    "    lr=learning_rate,           # learning rate (from hyperparameters)\n",
    "    log_steps=log_steps,        # print loss every N steps\n",
    "    sample_interval=sample_interval,  # seconds between text samples\n",
    "    max_steps_per_epoch=max_steps_per_epoch,  # or None for full epoch\n",
    "    enc=enc,                    # tokenizer\n",
    "    prompt=default_prompt       # prompt for text generation samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4ff956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 3.7234840393066406\n",
      "Perplexity: 41.40841293334961\n"
     ]
    }
   ],
   "source": [
    "# Evaluate loss on a batch for any model (KGramMLPSeqModel, LSTMSeqModel, TransformerModel)\n",
    "# Evaluate loss and perplexity on a batch for any model (KGramMLPSeqModel, LSTMSeqModel, TransformerModel)\n",
    "try:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens in train_loader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            logits = model(batch_tokens)\n",
    "            loss = compute_next_token_loss(logits, batch_tokens)\n",
    "            print(\"Batch loss:\", loss.item())\n",
    "            # Optionally, compute perplexity\n",
    "            perplexity = torch.exp(loss)\n",
    "            print(\"Perplexity:\", perplexity.item())\n",
    "            break  # Remove break to check more batches\n",
    "except Exception as e:\n",
    "    print(f'Error during evaluation: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6969c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Once upon a time, there was a big kids. has a dangerousie ands and a new boat. Tweetieie. One day, the colorful as a little boy came to walking everywhere. The different bit deep. He knew that he was ready with feeling all\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate text from a trained model\n",
    "prompt = \"Once upon a time\"\n",
    "max_new_tokens = 50  # Number of tokens to generate\n",
    "\n",
    "# For TransformerModel, you can use top_p for nucleus sampling or greedy (top_p=None)\n",
    "final_text, annotated_text = generate_text(\n",
    "    model,         # your trained model\n",
    "    enc,           # your tokenizer (e.g., tiktoken.get_encoding(\"gpt2\"))\n",
    "    prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    device=device, # your torch.device\n",
    "    top_p=0.95,    # or None for greedy\n",
    "    use_kv_cache=False  # True for TransformerModel if you want fast generation\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(final_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
