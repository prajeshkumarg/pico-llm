{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7f9b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gprajeshkumar/Documents/NYU Courant/Machine Learning/hws/pico-llm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/gprajeshkumar/Documents/NYU Courant/Machine Learning/hws/pico-llm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gprajeshkumar/Documents/NYU Courant/Machine Learning/hws/pico-llm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/gprajeshkumar/Documents/NYU Courant/Machine Learning/hws/pico-llm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "\n",
    "print('All imports done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3af9ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8c461",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "For Data\n",
    "1. block_size\n",
    "2. train_subset_size\n",
    "3. batch_size\n",
    "4. tinystories_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5735ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "block_size = 128  # Sequence length for training\n",
    "train_subset_size = 20000  # Number of samples to use from TinyStories\n",
    "batch_size = 16\n",
    "\n",
    "tinystories_weight = 0.5  # Proportion of TinyStories in mixed dataset (0.0 to skip)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "log_steps = 100\n",
    "sample_interval = 30  # seconds between text samples during training\n",
    "max_steps_per_epoch = None  # Set to an int for quick tests\n",
    "\n",
    "transformer_d_model = 256\n",
    "transformer_n_heads = 4\n",
    "transformer_n_blocks = 2\n",
    "transformer_max_seq_len = block_size\n",
    "\n",
    "\n",
    "# Prompt for text generation\n",
    "default_prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc1dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories from huggingface with weight=0.5...\n",
      "TinyStories sequences: 20000\n",
      "TinyStories sequences: 20000\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = enc.n_vocab\n",
    "\n",
    "# Load TinyStories dataset\n",
    "tinystories_seqs = []\n",
    "if tinystories_weight > 0.0:\n",
    "    print(f\"Loading TinyStories from huggingface with weight={tinystories_weight}...\")\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    train_dataset = dataset.select(range(train_subset_size))\n",
    "    for sample in train_dataset:\n",
    "        text = sample['text']\n",
    "        tokens = enc.encode(text)\n",
    "        tokens = tokens[:block_size]\n",
    "        if len(tokens) > 0:\n",
    "            tinystories_seqs.append(tokens)\n",
    "    print(f\"TinyStories sequences: {len(tinystories_seqs)}\")\n",
    "else:\n",
    "    print(\"TinyStories weight=0 => skipping TinyStories.\")\n",
    "other_seqs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f926891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tinystories_seqs, other_seqs, p_tiny: float):\n",
    "        super().__init__()\n",
    "        self.tinystories_seqs = tinystories_seqs\n",
    "        self.other_seqs = other_seqs\n",
    "        self.p_tiny = p_tiny\n",
    "        self.has_tinystories = (len(self.tinystories_seqs) > 0)\n",
    "        self.has_other = (len(self.other_seqs) > 0)\n",
    "        self.total_length = len(self.tinystories_seqs) + len(self.other_seqs)\n",
    "        if self.total_length == 0:\n",
    "            raise ValueError(\"No data found! Both TinyStories and other sets are empty.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import random\n",
    "        r = random.random()\n",
    "        if self.has_tinystories and self.has_other:\n",
    "            if r < self.p_tiny:\n",
    "                i = random.randint(0, len(self.tinystories_seqs) - 1)\n",
    "                seq = self.tinystories_seqs[i]\n",
    "            else:\n",
    "                i = random.randint(0, len(self.other_seqs) - 1)\n",
    "                seq = self.other_seqs[i]\n",
    "        elif self.has_tinystories:\n",
    "            i = random.randint(0, len(self.tinystories_seqs) - 1)\n",
    "            seq = self.tinystories_seqs[i]\n",
    "        else:\n",
    "            i = random.randint(0, len(self.other_seqs) - 1)\n",
    "            seq = self.other_seqs[i]\n",
    "        return torch.tensor(seq, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b498ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_collate_fn(batch):\n",
    "    max_len = max(len(seq) for seq in batch)\n",
    "    batch_size = len(batch)\n",
    "    padded = torch.zeros(max_len, batch_size, dtype=torch.long)\n",
    "    for i, seq in enumerate(batch):\n",
    "        seq_len = seq.size(0)\n",
    "        padded[:seq_len, i] = seq\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and loader\n",
    "p_tiny = tinystories_weight\n",
    "combined_dataset = MixedSequenceDataset(\n",
    "    tinystories_seqs=tinystories_seqs,\n",
    "    other_seqs=other_seqs,\n",
    "    p_tiny=p_tiny\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5bc3495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18531162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader ready. Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=seq_collate_fn\n",
    ")\n",
    "\n",
    "print(\"DataLoader ready. Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2b59ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories val subset (20000:22000)...\n",
      "TinyStories VAL sequences: 2000\n",
      "Validation DataLoader ready. Num val sequences: 2000\n",
      "TinyStories VAL sequences: 2000\n",
      "Validation DataLoader ready. Num val sequences: 2000\n"
     ]
    }
   ],
   "source": [
    "### Validation \n",
    "# Validation data (next 2000 samples)\n",
    "val_subset_size = train_subset_size // 10  # 2000\n",
    "tinystories_val_seqs = []\n",
    "print(f\"Loading TinyStories val subset ({train_subset_size}:{train_subset_size + val_subset_size})...\")\n",
    "dataset_val = dataset.select(range(train_subset_size, train_subset_size + val_subset_size))  # 20000..21999\n",
    "for sample in dataset_val:\n",
    "    text = sample[\"text\"]\n",
    "    tokens = enc.encode(text)\n",
    "    tokens = tokens[:block_size]\n",
    "    if len(tokens) > 0:\n",
    "        tinystories_val_seqs.append(tokens)\n",
    "print(f\"TinyStories VAL sequences: {len(tinystories_val_seqs)}\")\n",
    "\n",
    "# Create validation dataset and DataLoader\n",
    "\n",
    "val_p_tiny = tinystories_weight  # same mixing proportion as training\n",
    "\n",
    "val_dataset = MixedSequenceDataset(\n",
    "    tinystories_seqs=tinystories_val_seqs,\n",
    "    other_seqs=[],          # no other validation data for now\n",
    "    p_tiny=val_p_tiny\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=seq_collate_fn\n",
    ")\n",
    "\n",
    "print(\"Validation DataLoader ready. Num val sequences:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b72f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_token_loss(logits, tokens_seq):\n",
    "    '''\n",
    "    logits: (seq_len, batch, vocab_size)\n",
    "    tokens_seq: (seq_len, batch)\n",
    "    Computes cross-entropy loss for next-token prediction.\n",
    "    '''\n",
    "    seq_len, batch = tokens_seq.shape\n",
    "    # Predict next token: input t => predict t+1\n",
    "    logits = logits[:-1]  # (seq_len-1, batch, vocab_size)\n",
    "    targets = tokens_seq[1:]  # (seq_len-1, batch)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7de17cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.use_bias = bias\n",
    "        self.bias = nn.Parameter(torch.zeros(dim)) if bias else None\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        y = (x / rms) * self.weight\n",
    "        if self.use_bias:\n",
    "            y = y + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480d06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention with optional KV cache.\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def _split_heads(self, t: torch.Tensor, T: int, B: int) -> torch.Tensor:\n",
    "        return t.view(T, B, self.n_heads, self.head_dim).permute(1, 2, 0, 3)  # (B,H,T,D)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv=None):\n",
    "        T, B, C = x.shape\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = self._split_heads(q, T, B)\n",
    "        k = self._split_heads(k, T, B)\n",
    "        v = self._split_heads(v, T, B)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            k_cache, v_cache = past_kv\n",
    "            if k_cache is not None:\n",
    "                k = torch.cat([k_cache, k], dim=2)\n",
    "                v = torch.cat([v_cache, v], dim=2)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B,H,T,T_total)\n",
    "        if past_kv is None and T > 1:\n",
    "            causal_mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        ctx = torch.matmul(attn_probs, v)  # (B,H,T,D)\n",
    "        ctx = ctx.permute(2, 0, 1, 3).contiguous().view(T, B, C)\n",
    "        out = self.o_proj(ctx)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            return out, (k, v)\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af02913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single decoder block: RMSNorm -> CausalAttn (residual) -> RMSNorm -> MLP (residual).\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads)\n",
    "        self.mlp_norm = RMSNorm(d_model)\n",
    "        inner = int(mlp_ratio * d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, inner, bias=False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(inner, d_model, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv=None):\n",
    "        if past_kv is None:\n",
    "            x = x + self.attn(self.attn_norm(x))\n",
    "        else:\n",
    "            attn_out, new_kv = self.attn(self.attn_norm(x), past_kv=past_kv)\n",
    "            x = x + attn_out\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "        if past_kv is not None:\n",
    "            return x, new_kv\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51439712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Decoder-only causal Transformer producing logits for next-token prediction.\"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size: int = 50257,\n",
    "                 d_model: int = 512,\n",
    "                 n_heads: int = 8,\n",
    "                 n_blocks: int = 6,\n",
    "                 max_seq_len: int = 2048,\n",
    "                 mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, mlp_ratio) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embed.weight  # Weight tying\n",
    "\n",
    "    def forward(self, tokens_seq: torch.Tensor, kv_cache=None):\n",
    "        T, B = tokens_seq.shape\n",
    "        if T > self.max_seq_len:\n",
    "            tokens_seq = tokens_seq[-self.max_seq_len:]\n",
    "            T = tokens_seq.shape[0]\n",
    "        if kv_cache is not None and len(kv_cache) > 0 and kv_cache[0][0] is not None:\n",
    "            cached_len = kv_cache[0][0].size(2)\n",
    "        else:\n",
    "            cached_len = 0\n",
    "        pos = torch.arange(cached_len, cached_len + T, device=tokens_seq.device)\n",
    "        x = self.token_embed(tokens_seq) + self.pos_embed(pos).unsqueeze(1)\n",
    "        new_cache = [] if kv_cache is not None else None\n",
    "        if kv_cache is None:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "        else:\n",
    "            for blk, past in zip(self.blocks, kv_cache):\n",
    "                x, updated = blk(x, past_kv=past)\n",
    "                new_cache.append(updated)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if kv_cache is not None:\n",
    "            return logits, new_cache\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb2ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(logits, p=0.95):\n",
    "    # Convert logits to probabilities\n",
    "    prob_dist = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Sort probabilities in descending order and get indices\n",
    "    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True)\n",
    "\n",
    "    # Compute cumulative sum\n",
    "    cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # Find the cutoff index where cumulative probability exceeds p\n",
    "    # We want to include all tokens up to where cumsum first exceeds p\n",
    "    cutoff_mask = cumsum_probs <= p\n",
    "\n",
    "    # Always include at least the first token (highest probability)\n",
    "    # This handles edge case where first token alone has prob > p\n",
    "    cutoff_mask[0] = True\n",
    "\n",
    "    # Zero out probabilities beyond the nucleus\n",
    "    filtered_probs = sorted_probs.clone()\n",
    "    filtered_probs[~cutoff_mask] = 0.0\n",
    "\n",
    "    # Renormalize the remaining probabilities\n",
    "    filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "\n",
    "    # Sample from the filtered distribution\n",
    "    sampled_index = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "\n",
    "    # Map back to original token index\n",
    "    chosen_token = sorted_indices[sampled_index].item()\n",
    "\n",
    "    return chosen_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc2016e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, enc, init_text, max_new_tokens=20, device=\"cpu\",\n",
    "                  top_p=None,\n",
    "                  monosemantic_info=None,\n",
    "                  do_monosemantic=False,\n",
    "                  use_kv_cache=False):\n",
    "    \"\"\"\n",
    "    A single code path for all models:\n",
    "      - We keep a growing list 'context_tokens'.\n",
    "      - At each step, we feed the entire context as (seq_len,1) to model(...).\n",
    "      - We get model(...)->(seq_len,1,vocab_size). We take the final step's logits => logits[-1,0,:].\n",
    "      - We pick next token (greedy or top-p), append to context_tokens.\n",
    "      - Optionally do monosemantic analysis on that newly generated token.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        context_tokens = enc.encode(init_text)\n",
    "        annotation_list = []\n",
    "\n",
    "        kv_cache = None\n",
    "        # Prime KV cache step-by-step over the initial prompt for strict causality\n",
    "        if use_kv_cache and hasattr(model, 'blocks') and len(context_tokens) > 0:\n",
    "            kv_cache = [(None, None) for _ in range(len(model.blocks))]\n",
    "            for tid in context_tokens:\n",
    "                tok = torch.tensor([tid], dtype=torch.long, device=device).unsqueeze(1)  # (1,1)\n",
    "                _, kv_cache = model(tok, kv_cache=kv_cache)\n",
    "\n",
    "        for step_i in range(max_new_tokens):\n",
    "            if use_kv_cache and hasattr(model, 'blocks'):\n",
    "                # Use only the last token and advance cache\n",
    "                if len(context_tokens) == 0:\n",
    "                    # Fallback: no context, create a space token as a starter\n",
    "                    last_id = enc.encode(\" \")[-1]\n",
    "                else:\n",
    "                    last_id = context_tokens[-1]\n",
    "                last_token = torch.tensor([last_id], dtype=torch.long, device=device).unsqueeze(1)  # (1,1)\n",
    "                logits_seq, kv_cache = model(last_token, kv_cache=kv_cache if kv_cache is not None else [(None, None) for _ in range(len(model.blocks))])\n",
    "                next_logits = logits_seq[-1, 0, :]\n",
    "            else:\n",
    "                # Fallback: full context each step (works for all models)\n",
    "                seq_tensor = torch.tensor(context_tokens, dtype=torch.long, device=device).unsqueeze(1)\n",
    "                logits_seq = model(seq_tensor)              # (seq_len,1,vocab_size)\n",
    "                next_logits = logits_seq[-1, 0, :]         # shape (vocab_size,)\n",
    "\n",
    "            if top_p is None:\n",
    "                # greedy\n",
    "                chosen_token = torch.argmax(next_logits).item()\n",
    "            else:\n",
    "                chosen_token = nucleus_sampling(next_logits, p=top_p)\n",
    "\n",
    "            context_tokens.append(chosen_token)\n",
    "\n",
    "            if do_monosemantic and monosemantic_info is not None:\n",
    "                neighbors = monosemantic_analysis_for_token(\n",
    "                    chosen_token, model, monosemantic_info, enc, device=device, top_n=5\n",
    "                )\n",
    "                annotation_list.append((chosen_token, neighbors))\n",
    "            else:\n",
    "                annotation_list.append((chosen_token, []))\n",
    "\n",
    "    model.train(was_training)\n",
    "\n",
    "    final_text = enc.decode(context_tokens)\n",
    "    prefix_text = enc.decode(context_tokens[:-max_new_tokens])\n",
    "    annotated_strs = [prefix_text]\n",
    "    for (tid, neighs) in annotation_list:\n",
    "        token_str = enc.decode([tid])\n",
    "        if neighs:\n",
    "            neighbor_strs = [f\"{enc.decode([x[1]])}\" for x in neighs]\n",
    "            annotated = f\"{token_str}[NN={neighbor_strs}]\"\n",
    "        else:\n",
    "            annotated = token_str\n",
    "        annotated_strs.append(annotated)\n",
    "\n",
    "    annotated_text = \"\".join(annotated_strs)\n",
    "    return final_text, annotated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96376414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(model,\n",
    "                    loader,\n",
    "                    epochs,\n",
    "                    model_name,\n",
    "                    device,\n",
    "                    lr=1e-3,\n",
    "                    log_steps=100,\n",
    "                    sample_interval=30,\n",
    "                    max_steps_per_epoch=None,\n",
    "                    enc=None,\n",
    "                    monosemantic_info=None,\n",
    "                    prompt=\"Once upon a\",\n",
    "                    log_csv_path: str = \"\",\n",
    "                    log_flush_steps: int = 100,\n",
    "                    val_loader=None,\n",
    "                    val_log_csv_path: str = \"\",\n",
    "                    val_interval_steps: int = None):\n",
    "    \"\"\"\n",
    "    Train the model and optionally run/record validation aligned to training global_step.\n",
    "\n",
    "    If val_loader and val_log_csv_path are provided, a validation pass is run\n",
    "    every `val_interval_steps` training steps (or once per epoch if\n",
    "    val_interval_steps is None). Validation loss is logged with the same\n",
    "    schema as training: timestamp,model,epoch,step_in_epoch,global_step,loss\n",
    "    and model name \"Transformer_val\".\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Buffered training logging setup\n",
    "    loss_buffer = []\n",
    "    csv_file = None\n",
    "    if log_csv_path:\n",
    "        log_dir = os.path.dirname(log_csv_path)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "        file_exists = os.path.exists(log_csv_path)\n",
    "        csv_file = open(log_csv_path, 'a', newline='')\n",
    "        # Write header if empty\n",
    "        if not file_exists or os.path.getsize(log_csv_path) == 0:\n",
    "            csv_file.write('timestamp,model,epoch,step_in_epoch,global_step,loss\\n')\n",
    "\n",
    "    # Validation logging setup (separate CSV)\n",
    "    val_csv_file = None\n",
    "    if val_loader is not None and val_log_csv_path:\n",
    "        val_log_dir = os.path.dirname(val_log_csv_path)\n",
    "        if val_log_dir:\n",
    "            os.makedirs(val_log_dir, exist_ok=True)\n",
    "        val_file_exists = os.path.exists(val_log_csv_path)\n",
    "        val_csv_file = open(val_log_csv_path, 'a', newline='')\n",
    "        if not val_file_exists or os.path.getsize(val_log_csv_path) == 0:\n",
    "            val_csv_file.write('timestamp,model,epoch,step_in_epoch,global_step,loss\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    next_sample_time = start_time\n",
    "    global_step = 0\n",
    "\n",
    "    def run_validation(current_epoch, current_global_step):\n",
    "        \"\"\"Run one full validation pass and log a single avg-loss point.\"\"\"\n",
    "        if val_loader is None or val_csv_file is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for _, batch_tokens in enumerate(val_loader, start=1):\n",
    "                batch_tokens = batch_tokens.to(device)\n",
    "                logits = model(batch_tokens)\n",
    "                vloss = compute_next_token_loss(logits, batch_tokens)\n",
    "                val_losses.append(vloss.item())\n",
    "        if not val_losses:\n",
    "            return\n",
    "        avg_val_loss = float(sum(val_losses) / len(val_losses))\n",
    "        print(f\"[Validation] Epoch {current_epoch}, global_step {current_global_step}, avg loss: {avg_val_loss:.4f}\")\n",
    "        # Log as one row aligned with training global_step\n",
    "        val_csv_file.write(\n",
    "            f\"{time.time()},Transformer_val,{current_epoch},1,{current_global_step},{avg_val_loss}\\n\"\n",
    "        )\n",
    "        val_csv_file.flush()\n",
    "        model.train()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        partial_loss = 0.0\n",
    "        partial_count = 0\n",
    "\n",
    "        step_in_epoch = 0\n",
    "        for batch_idx, batch_tokens in enumerate(loader, start=1):\n",
    "            step_in_epoch += 1\n",
    "            global_step += 1\n",
    "\n",
    "            batch_tokens = batch_tokens.to(device)  # (seq_len, batch)\n",
    "\n",
    "            logits = model(batch_tokens)  # (seq_len, batch, vocab_size)\n",
    "            loss = compute_next_token_loss(logits, batch_tokens)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            partial_loss += loss.item()\n",
    "            partial_count += 1\n",
    "\n",
    "            # Buffer this step's loss\n",
    "            if csv_file is not None:\n",
    "                loss_buffer.append(f\"{time.time()},{model_name},{epoch},{step_in_epoch},{global_step},{loss.item()}\\n\")\n",
    "                if len(loss_buffer) >= log_flush_steps:\n",
    "                    csv_file.writelines(loss_buffer)\n",
    "                    csv_file.flush()\n",
    "                    loss_buffer.clear()\n",
    "\n",
    "            # Periodic training progress print\n",
    "            if batch_idx % log_steps == 0:\n",
    "                avg_part_loss = partial_loss / partial_count\n",
    "                print(f\"[{model_name}] Epoch {epoch}/{epochs}, \"\n",
    "                      f\"Step {batch_idx}/{len(loader)} (global step: {global_step}) \"\n",
    "                      f\"Partial Avg Loss: {avg_part_loss:.4f}\")\n",
    "                partial_loss = 0.0\n",
    "                partial_count = 0\n",
    "\n",
    "            # Periodic text sampling\n",
    "            current_time = time.time()\n",
    "            if current_time >= next_sample_time and enc is not None:\n",
    "                with torch.no_grad():\n",
    "                    print(f\"\\n[{model_name}] Generating sample text (greedy) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_greedy, ann_greedy = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=None,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Greedy Sample: {text_greedy}\")\n",
    "                    print(f\" Annotated: {ann_greedy}\\n\")\n",
    "\n",
    "                    print(f\"[{model_name}] Generating sample text (top-p=0.95) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_topp, ann_topp = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=0.95,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Top-p (p=0.95) Sample: {text_topp}\")\n",
    "                    print(f\" Annotated: {ann_topp}\\n\")\n",
    "\n",
    "                    print(f\"[{model_name}] Generating sample text (top-p=1.0) at epoch={epoch}, step={batch_idx}...\")\n",
    "                    text_topp1, ann_topp1 = generate_text(\n",
    "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
    "                        top_p=1.0,\n",
    "                        use_kv_cache=False,\n",
    "                        monosemantic_info=monosemantic_info,\n",
    "                        do_monosemantic=(monosemantic_info is not None)\n",
    "                    )\n",
    "                    print(f\" Top-p (p=1.0) Sample: {text_topp1}\")\n",
    "                    print(f\" Annotated: {ann_topp1}\\n\")\n",
    "\n",
    "                next_sample_time = current_time + sample_interval\n",
    "\n",
    "            # Run validation either every N steps or once per epoch if val_interval_steps is None\n",
    "            if val_loader is not None and val_log_csv_path:\n",
    "                if val_interval_steps is not None:\n",
    "                    if global_step % val_interval_steps == 0:\n",
    "                        run_validation(epoch, global_step)\n",
    "                else:\n",
    "                    # If no interval specified, run once at the last batch of the epoch\n",
    "                    if max_steps_per_epoch is None:\n",
    "                        is_last_batch = (batch_idx == len(loader))\n",
    "                    else:\n",
    "                        is_last_batch = (step_in_epoch >= max_steps_per_epoch or batch_idx == len(loader))\n",
    "                    if is_last_batch:\n",
    "                        run_validation(epoch, global_step)\n",
    "\n",
    "            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:\n",
    "                print(f\"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, ending epoch {epoch} early.\")\n",
    "                break\n",
    "\n",
    "        avg_loss = total_loss / step_in_epoch\n",
    "        print(f\"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Final flush for training log\n",
    "    if csv_file is not None:\n",
    "        if loss_buffer:\n",
    "            csv_file.writelines(loss_buffer)\n",
    "            csv_file.flush()\n",
    "        csv_file.close()\n",
    "\n",
    "    # Close validation log if open\n",
    "    if val_csv_file is not None:\n",
    "        val_csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29baffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (token_embed): Embedding(50257, 256)\n",
      "  (pos_embed): Embedding(128, 256)\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (attn_norm): RMSNorm()\n",
      "      (attn): CausalSelfAttention(\n",
      "        (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "      )\n",
      "      (mlp_norm): RMSNorm()\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=False)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Transformer model\n",
    "model = TransformerModel(vocab_size=vocab_size, d_model=512, n_heads=4, n_blocks=2, max_seq_len=block_size)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5963c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1...\n",
      " Greedy Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...\n",
      " Greedy Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...\n",
      " Top-p (p=0.95) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1...\n",
      " Top-p (p=0.95) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1...\n",
      " Top-p (p=1.0) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=53...\n",
      " Greedy Sample: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      " Annotated: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=53...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=53...\n",
      " Greedy Sample: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      " Annotated: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=53...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      " Annotated: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=53...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      " Annotated: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=53...\n",
      " Top-p (p=1.0) Sample: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      " Annotated: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      " Annotated: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      "\n",
      "[Transformer] Epoch 1/3, Step 100/1250 (global step: 100) Partial Avg Loss: 28.0934\n",
      "[Transformer] Epoch 1/3, Step 100/1250 (global step: 100) Partial Avg Loss: 28.0934\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=107...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=107...\n",
      " Greedy Sample: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Annotated: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=107...\n",
      " Greedy Sample: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Annotated: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=107...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      " Annotated: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=107...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      " Annotated: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=107...\n",
      " Top-p (p=1.0) Sample: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      " Annotated: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      " Annotated: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=170...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=170...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=170...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=170...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      " Annotated: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=170...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      " Annotated: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=170...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      " Annotated: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      " Annotated: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=1...\n",
      " Greedy Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...\n",
      " Greedy Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...\n",
      " Top-p (p=0.95) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1...\n",
      " Top-p (p=0.95) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=1...\n",
      " Top-p (p=1.0) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      " Annotated: Once upon a time time time time time time time time time time time time time time time time time time time time time\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=53...\n",
      " Greedy Sample: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      " Annotated: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=53...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=53...\n",
      " Greedy Sample: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      " Annotated: Once upon a time, there there there there there there there there there there there there there there there there there there there\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=53...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      " Annotated: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=53...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      " Annotated: Once upon a time, there\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she. a to Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick Fitzpatrick232232\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=53...\n",
      " Top-p (p=1.0) Sample: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      " Annotated: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      " Annotated: Once upon a timeREF was a there there there there scored. be be be be be a,smartlettelettelette\n",
      "\n",
      "[Transformer] Epoch 1/3, Step 100/1250 (global step: 100) Partial Avg Loss: 28.0934\n",
      "[Transformer] Epoch 1/3, Step 100/1250 (global step: 100) Partial Avg Loss: 28.0934\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=107...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=107...\n",
      " Greedy Sample: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Annotated: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=107...\n",
      " Greedy Sample: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Annotated: Once upon a time, there was a little.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=107...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      " Annotated: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=107...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      " Annotated: Once upon a time, there sad find. She the I I looked on was and her her wanted wanted to Lily a\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=107...\n",
      " Top-p (p=1.0) Sample: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      " Annotated: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      " Annotated: Once upon a time time Tim Tim a scored idea\n",
      " bit bit's were cl in the play and said dolls. with\n",
      "\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=170...\n",
      "\n",
      "[Transformer] Generating sample text (greedy) at epoch=1, step=170...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=170...\n",
      " Greedy Sample: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      " Annotated: Once upon a time, there was a little girl named Lily. She. She was very very very very. One day\n",
      "\n",
      "[Transformer] Generating sample text (top-p=0.95) at epoch=1, step=170...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      " Annotated: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=170...\n",
      " Top-p (p=0.95) Sample: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      " Annotated: Once upon a time, there was a little'm.\".\" bed. One wanted to Inside was silly App the dolls to\n",
      "\n",
      "[Transformer] Generating sample text (top-p=1.0) at epoch=1, step=170...\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      " Annotated: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      "\n",
      " Top-p (p=1.0) Sample: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      " Annotated: Once upon a time, there was a little dinner and or. loved loved. It with it. ase fun rabbit\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_one_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# your DataLoader\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# number of epochs (from hyperparameters)\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# name for logging\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# torch.device (CPU or MPS)\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# learning rate (from hyperparameters)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# print loss every N steps\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# seconds between text samples\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or None for full epoch\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# tokenizer\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# prompt for text generation samples\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_logs/transformer_training_log.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_log_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_logs/transformer_validation_log.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_interval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m   \n",
      "Cell \u001b[0;32mIn[18], line 100\u001b[0m, in \u001b[0;36mtrain_one_model\u001b[0;34m(model, loader, epochs, model_name, device, lr, log_steps, sample_interval, max_steps_per_epoch, enc, monosemantic_info, prompt, log_csv_path, log_flush_steps, val_loader, val_log_csv_path, val_interval_steps)\u001b[0m\n\u001b[1;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 100\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m partial_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    102\u001b[0m partial_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_one_model(\n",
    "    model=model,                \n",
    "    loader=train_loader,        # your DataLoader\n",
    "    epochs=epochs,              # number of epochs (from hyperparameters)\n",
    "    model_name=\"Transformer\",   # name for logging\n",
    "    device=device,              # torch.device (CPU or MPS)\n",
    "    lr=learning_rate,           # learning rate (from hyperparameters)\n",
    "    log_steps=log_steps,        # print loss every N steps\n",
    "    sample_interval=sample_interval,  # seconds between text samples\n",
    "    max_steps_per_epoch=max_steps_per_epoch,  # or None for full epoch\n",
    "    enc=enc,                    # tokenizer\n",
    "    prompt=default_prompt,       # prompt for text generation samples\n",
    "    log_csv_path=\"training_logs/transformer_training_log.csv\",\n",
    "    val_loader=val_loader,\n",
    "    val_log_csv_path=\"training_logs/transformer_validation_log.csv\",\n",
    "    val_interval_steps=log_steps * 2\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens in train_loader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            logits = model(batch_tokens)\n",
    "            loss = compute_next_token_loss(logits, batch_tokens)\n",
    "            print(\"Batch loss:\", loss.item())\n",
    "            # Optionally, compute perplexity\n",
    "            perplexity = torch.exp(loss)\n",
    "            print(\"Perplexity:\", perplexity.item())\n",
    "            break  # Remove break to check more batches\n",
    "except Exception as e:\n",
    "    print(f'Error during evaluation: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d09d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate text from a trained model\n",
    "prompt = \"There was once a\"\n",
    "max_new_tokens = 50  # Number of tokens to generate\n",
    "\n",
    "\n",
    "final_text, annotated_text = generate_text(\n",
    "    model,         # your trained model\n",
    "    enc,           # your tokenizer (e.g., tiktoken.get_encoding(\"gpt2\"))\n",
    "    prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    device=device, # your torch.device\n",
    "    top_p=0.95,    # or None for greedy\n",
    "    use_kv_cache=False  # True for TransformerModel if you want fast generation\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths for training and validation logs\n",
    "train_log_path = \"training_logs/transformer_training_log.csv\"\n",
    "val_log_path = \"training_logs/transformer_validation_log.csv\"\n",
    "\n",
    "if os.path.exists(train_log_path):\n",
    "    # Load training CSV (timestamp,model,epoch,step_in_epoch,global_step,loss)\n",
    "    train_data = np.genfromtxt(train_log_path, delimiter=\",\", skip_header=1)\n",
    "    train_global_steps = train_data[:, 4]\n",
    "    train_losses = train_data[:, 5]\n",
    "else:\n",
    "    train_global_steps, train_losses = None, None\n",
    "    print(f\"No training log found at: {train_log_path}. Set log_csv_path in train_one_model to save losses.\")\n",
    "\n",
    "if os.path.exists(val_log_path):\n",
    "    # Load validation CSV with same schema\n",
    "    val_data = np.genfromtxt(val_log_path, delimiter=\",\", skip_header=1)\n",
    "    val_global_steps = val_data[:, 4]\n",
    "    val_losses = val_data[:, 5]\n",
    "else:\n",
    "    val_global_steps, val_losses = None, None\n",
    "    print(f\"No validation log found at: {val_log_path}. Run the validation cell to create it.\")\n",
    "\n",
    "if train_global_steps is not None or val_global_steps is not None:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    if train_global_steps is not None:\n",
    "        plt.plot(train_global_steps, train_losses, label=\"Training\", alpha=0.7)\n",
    "\n",
    "    if val_global_steps is not None:\n",
    "        plt.plot(val_global_steps, val_losses, label=\"Validation\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"Global Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss over Time\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if train_losses is not None:\n",
    "        print(f\"Training: initial={train_losses[0]:.3f}, final={train_losses[-1]:.3f}, drop={(train_losses[0]-train_losses[-1])/train_losses[0]*100:.1f}%\")\n",
    "    if val_losses is not None:\n",
    "        print(f\"Validation: initial={val_losses[0]:.3f}, final={val_losses[-1]:.3f}, drop={(val_losses[0]-val_losses[-1])/val_losses[0]*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
